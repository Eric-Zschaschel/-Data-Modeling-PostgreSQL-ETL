{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The official documentation for PostgreSQL features an entire section on [Populating a Database](https://www.postgresql.org/docs/current/populate.html#POPULATE-COPY-FROM). According to the documentation, the best way to load data into a database is using the `copy` command - this is much faster than the INSERT. Therefore I created this etl to do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from typing import Iterator, Dict, Any, Optional\n",
    "from sql_queries import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=postgres password=great\")\n",
    "cur = conn.cursor()\n",
    "conn.set_session(autocommit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: postgres@sparkifydb'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql+psycopg2://postgres:great@localhost/sparkifydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://postgres:***@localhost/sparkifydb\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>7752</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(7752,)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "SELECT COUNT(*) FROM songplays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files matching extension from directory\n",
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    # walk() generates the file names in a directory tree\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        # glob finds all the pathnames matching a specified pattern\n",
    "        # join combines the two path elements\n",
    "        files = glob.glob(os.path.join(root, '*.json'))\n",
    "        for f in files:\n",
    "            # add the absolute path to the list\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The String Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thanks to the [beer iterator](https://hakibenita.com/fast-load-data-python-postgresql#copy-data-from-a-string-iterator)\n",
    "the following class creates a file-like object that will act as a buffer between the remote source and the COPY command. The buffer will consume JSON via the iterator, clean and transform the data, and output clean CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringIteratorIO(io.TextIOBase):\n",
    "    def __init__(self, iter: Iterator[str]):\n",
    "        self._iter = iter\n",
    "        self._buff = ''\n",
    "\n",
    "    def readable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _read1(self, n: Optional[int] = None) -> str:\n",
    "        while not self._buff:\n",
    "            try:\n",
    "                self._buff = next(self._iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        ret = self._buff[:n]\n",
    "        self._buff = self._buff[len(ret):]\n",
    "        return ret\n",
    "\n",
    "    def read(self, n: Optional[int] = None) -> str:\n",
    "        line = []\n",
    "        if n is None or n < 0:\n",
    "            while True:\n",
    "                m = self._read1()\n",
    "                if not m:\n",
    "                    break\n",
    "                line.append(m)\n",
    "        else:\n",
    "            while n > 0:\n",
    "                m = self._read1(n)\n",
    "                if not m:\n",
    "                    break\n",
    "                n -= len(m)\n",
    "                line.append(m)\n",
    "        return ''.join(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, it fetches the rows from only when its internal line buffer is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Values\n",
    "Empty values are transformed to `\\N`. It is the default string used by PostgreSQL to indicate NULL in `COPY` (this can be changed using the NULL option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_value(value: Optional[Any]) -> str:\n",
    "    if value is None:\n",
    "        return r'\\N'\n",
    "    if value == 'NaN':\n",
    "        return r'\\N'\n",
    "    return str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The json file generator\n",
    "Create a generator that reads a list of data paths and loads each json file as a dictionary.\n",
    "If a json file has multiple dictionaries inside, it yields them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_gen(file_list: list) -> Iterator[Dict[str, Any]]:\n",
    "    import json\n",
    "    for file in file_list:    \n",
    "        with open(file) as json_file: \n",
    "            data = []\n",
    "            for line in json_file:\n",
    "                data = json.loads(line)\n",
    "                if not data:\n",
    "                    break\n",
    "                yield data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json to PostgreSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(datapath: str) -> None:\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (\n",
    "            i['song_id'],\n",
    "            i['title'],\n",
    "            i['artist_id'],\n",
    "            i['year'],\n",
    "            i['duration'],\n",
    "            i['artist_name'],\n",
    "            i['artist_location'],\n",
    "            i['artist_latitude'],\n",
    "            i['artist_longitude']\n",
    "        ))) + '\\n'\n",
    "        for i in jsonfile if i['song_id'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DELETE FROM staging_songs;\"\"\")   \n",
    "    cur.copy_expert(\"COPY staging_songs FROM STDIN DELIMITER '|'\", x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event_data(datapath: str) -> None:\n",
    "    from datetime import datetime\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (          \n",
    "            datetime.fromtimestamp(i['ts']/1000.0),\n",
    "            i['userId'],\n",
    "            i['level'],\n",
    "            i['sessionId'],\n",
    "            i['location'],\n",
    "            i['userAgent'],\n",
    "            i['firstName'],\n",
    "            i['lastName'],\n",
    "            i['gender'],\n",
    "            i['song'],\n",
    "            i['artist'],\n",
    "            i['length']\n",
    "        ))) + '\\n'\n",
    "        for i in jsonfile if i['userId'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DELETE FROM staging_events;\"\"\")   \n",
    "    cur.copy_expert(\"COPY staging_events FROM STDIN DELIMITER '|'\", x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.normcase(os.getcwd()) + '/data/log_data'\n",
    "process_event_data(datapath)\n",
    "\n",
    "datapath = os.path.normcase(os.getcwd()) + '/data/song_data'\n",
    "process_song_data(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
    "                SELECT artist_id, name, location, latitude, longitude FROM staging_songs\n",
    "                ON CONFLICT (artist_id) DO NOTHING;\"\"\")\n",
    "cur.execute(\"\"\"INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
    "                SELECT song_id, title, artist_id, year, duration FROM staging_songs\n",
    "                ON CONFLICT (song_id) DO NOTHING;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
    "                SELECT \n",
    "                start_time, \n",
    "                EXTRACT(hour FROM start_time),\n",
    "                EXTRACT(day FROM start_time),\n",
    "                EXTRACT(week FROM start_time),\n",
    "                EXTRACT(month FROM start_time),\n",
    "                EXTRACT(year FROM start_time),\n",
    "                EXTRACT(dow FROM start_time)\n",
    "                FROM staging_events\n",
    "                ON CONFLICT (start_time) DO NOTHING;\"\"\")\n",
    "cur.execute(\"\"\"INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
    "                SELECT user_id, first_name, last_name, gender, level \n",
    "                FROM staging_events \n",
    "                WHERE user_id IS NOT NULL AND user_id <> 0\n",
    "                ON CONFLICT (user_id) DO NOTHING;\"\"\")\n",
    "cur.execute(\"\"\"INSERT INTO songplays (start_time, user_id, level, \n",
    "                song_id, artist_id, session_id, location, user_agent) \n",
    "                SELECT g.start_time, g.user_id, g.level, h.song_id, \n",
    "                h.artist_id, g.session_id, g.location, g.user_agent \n",
    "                FROM staging_events g\n",
    "                LEFT JOIN (\n",
    "                    SELECT song_id, j.artist_id, k.name as artist, title, \n",
    "                    duration FROM songs j INNER JOIN artists k \n",
    "                    ON j.artist_id = k.artist_id) h ON g.song = h.title \n",
    "                    AND g.length = h.duration AND g.artist = h.artist\n",
    "                ON CONFLICT (start_time, user_id) DO NOTHING;\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Connection to Sparkify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
