{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Utilities\n",
    "Additional functions and data for `etl.py` file to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from typing import Iterator, Dict, Any, Optional\n",
    "from sql_queries import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=student password=student\")\n",
    "cur = conn.cursor()\n",
    "conn.set_session(autocommit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get all files matching extension from directory\n",
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    # walk() generates the file names in a directory tree\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        # glob finds all the pathnames matching a specified pattern\n",
    "        # join combines the two path elements\n",
    "        files = glob.glob(os.path.join(root, '*.json'))\n",
    "        for f in files:\n",
    "            # add the absolute path to the list\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# #1: copy\n",
    "The official documentation for PostgreSQL features an entire section on [Populating a Database](https://www.postgresql.org/docs/current/populate.html#POPULATE-COPY-FROM). According to the documentation, the best way to load data into a database is using the `copy` command.\n",
    "\n",
    "thanks to the [beer iterator](https://hakibenita.com/fast-load-data-python-postgresql#copy-data-from-a-string-iterator)\n",
    "the following class creates a file-like object that will act as a buffer between the remote source and the COPY command. The buffer will consume JSON via the iterator, clean and transform the data, and output clean CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StringIteratorIO(io.TextIOBase):\n",
    "    def __init__(self, iter: Iterator[str]):\n",
    "        self._iter = iter\n",
    "        self._buff = ''\n",
    "\n",
    "    def readable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _read1(self, n: Optional[int] = None) -> str:\n",
    "        while not self._buff:\n",
    "            try:\n",
    "                self._buff = next(self._iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        ret = self._buff[:n]\n",
    "        self._buff = self._buff[len(ret):]\n",
    "        return ret\n",
    "\n",
    "    def read(self, n: Optional[int] = None) -> str:\n",
    "        line = []\n",
    "        if n is None or n < 0:\n",
    "            while True:\n",
    "                m = self._read1()\n",
    "                if not m:\n",
    "                    break\n",
    "                line.append(m)\n",
    "        else:\n",
    "            while n > 0:\n",
    "                m = self._read1(n)\n",
    "                if not m:\n",
    "                    break\n",
    "                n -= len(m)\n",
    "                line.append(m)\n",
    "        return ''.join(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Internally, it fetches the rows from only when its internal line buffer is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### copy - Transforms a single value\n",
    "Empty values are transformed to `\\N`: The string `\\N` is the default string used by PostgreSQL to indicate NULL in `COPY` (this can be changed using the NULL option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_csv_value(value: Optional[Any]) -> str:\n",
    "    if value is None:\n",
    "        return r'\\N'\n",
    "    if value == 'NaN':\n",
    "        return r'\\N'\n",
    "    return str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The json file generator\n",
    "Create a generator that reads a list of data paths and loads each json file as a dictionary.\n",
    "If a json file has multiple dictionaries inside, it yields them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def json_gen(file_list: list) -> Iterator[Dict[str, Any]]:\n",
    "    import json\n",
    "    for file in file_list:    \n",
    "        with open(file) as json_file: \n",
    "            data = []\n",
    "            for line in json_file:\n",
    "                data = json.loads(line)\n",
    "                if not data:\n",
    "                    break\n",
    "                yield data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### json to PostgreSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_songs(cur, conn, datapath: str) -> None:\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (\n",
    "            i['song_id'],\n",
    "            i['title'],\n",
    "            i['artist_id'],\n",
    "            i['year'],\n",
    "            i['duration']\n",
    "        ))) + '\\n'\n",
    "        for i in jsonfile if i['song_id'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_songs;\n",
    "                SELECT * INTO tmp_songs FROM songs;\"\"\")\n",
    "    cur.copy_from(x, 'tmp_songs', sep='|')\n",
    "    cur.execute(\"\"\"INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
    "                SELECT song_id, title, artist_id, year, duration FROM tmp_songs\n",
    "                ON CONFLICT (song_id) DO NOTHING;\n",
    "                DROP TABLE tmp_songs;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOINLJW12A8C13314C',\n",
       "  'City Slickers',\n",
       "  'AR8IEZO1187B99055E',\n",
       "  2008,\n",
       "  149.86404),\n",
       " ('SOGDBUF12A8C140FAA', 'Intro', 'AR558FS1187FB45658', 2003, 75.67628),\n",
       " ('SORRZGD12A6310DBC3',\n",
       "  'Harajuku Girls',\n",
       "  'ARVBRGZ1187FB4675A',\n",
       "  2004,\n",
       "  290.55955),\n",
       " ('SONWXQJ12A8C134D94',\n",
       "  'The Ballad Of Sleeping Beauty',\n",
       "  'ARNF6401187FB57032',\n",
       "  1994,\n",
       "  305.162),\n",
       " ('SOFCHDR12AB01866EF', 'Living Hell', 'AREVWGE1187B9B890A', 0, 282.43546)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getcwd returns current working directory\n",
    "#normcase normalizes the path because windows slashes\n",
    "datapath = os.path.normcase(os.getcwd()) + '/data/song_data'\n",
    "process_songs(cur, conn, datapath)\n",
    "\n",
    "cur.execute(\"select * from songs limit 5;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_artists(datapath: str) -> None:\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (\n",
    "            i['artist_id'],\n",
    "            i['artist_name'],\n",
    "            i['artist_location'],\n",
    "            i['artist_latitude'],\n",
    "            i['artist_longitude']\n",
    "        ))) + '\\n'\n",
    "        for i in jsonfile if i['artist_id'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_artists;\n",
    "                SELECT * INTO tmp_artists FROM artists;\"\"\")\n",
    "    cur.copy_from(x, 'tmp_artists', sep='|')\n",
    "    cur.execute(\"\"\"INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
    "                SELECT artist_id, name, location, latitude, longitude FROM tmp_artists\n",
    "                ON CONFLICT (artist_id) DO NOTHING;\n",
    "                DROP TABLE tmp_artists;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(69,)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getcwd returns current working directory\n",
    "#normcase normalizes the path because windows slashes\n",
    "datapath = os.path.normcase(os.getcwd()) + '/data/song_data'\n",
    "process_artists(datapath)\n",
    "\n",
    "cur.execute(\"SELECT COUNT(artist_id) from artists;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def json_gen_time(file_list: list) -> Dict[str, Any]:\n",
    "    for file in file_list:    \n",
    "        df = pd.read_json(file, lines=True)\n",
    "        df = df.loc[df['page'] == 'NextSong']\n",
    "        df['ts']= pd.to_datetime(df['ts'], unit = 'ms')\n",
    "        \n",
    "        t = df['ts']\n",
    "        t.drop_duplicates(inplace=True)\n",
    "        t.dropna(inplace=True)\n",
    "        \n",
    "        time_df = pd.DataFrame(index=t.index)\n",
    "        time_df['start_time'] = t\n",
    "        time_df['hour'] = t.dt.hour\n",
    "        time_df['day'] = t.dt.day\n",
    "        time_df['week'] = t.dt.weekofyear\n",
    "        time_df['month'] = t.dt.month\n",
    "        time_df['year'] = t.dt.year\n",
    "        time_df['weekday'] = t.dt.weekday\n",
    "\n",
    "        for index, row in time_df.iterrows():\n",
    "            data = row.to_dict()\n",
    "            yield data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_time(datapath: str) -> None:\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen_time(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (\n",
    "            i['start_time'],\n",
    "            i['hour'],\n",
    "            i['day'],\n",
    "            i['week'],\n",
    "            i['month'],\n",
    "            i['year'],\n",
    "            i['weekday']\n",
    "        ))) + '\\n'\n",
    "        for i in jsonfile if i['start_time'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_time;\n",
    "                SELECT * INTO tmp_time FROM time;\"\"\")\n",
    "    cur.copy_from(x, 'tmp_time', sep='|')\n",
    "    cur.execute(\"\"\"INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
    "                SELECT start_time, hour, day, week, month, year, weekday FROM tmp_time\n",
    "                ON CONFLICT (start_time) DO NOTHING;\n",
    "                DROP TABLE tmp_time;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6813,)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = os.path.normcase(os.getcwd()) + '/data/log_data'\n",
    "process_time(datapath)\n",
    "\n",
    "cur.execute(\"SELECT COUNT(start_time) from time;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_users(datapath: str) -> None:\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (\n",
    "            i['userId'],\n",
    "            i['firstName'],\n",
    "            i['lastName'],\n",
    "            i['gender'],\n",
    "            i['level']\n",
    "        ))) + '\\n'\n",
    "        for i in jsonfile if i['userId'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_users;\n",
    "                SELECT * INTO tmp_users FROM users;\"\"\")\n",
    "    cur.copy_from(x, 'tmp_users', sep='|')\n",
    "    cur.execute(\"\"\"INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
    "                SELECT user_id, first_name, last_name, gender, level FROM tmp_users \n",
    "                WHERE user_id IS NOT NULL AND user_id <> 0\n",
    "                ON CONFLICT (user_id) DO NOTHING;\n",
    "                DROP TABLE tmp_users;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(97,)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = os.path.normcase(os.getcwd()) + '/data/log_data'\n",
    "process_users(datapath)\n",
    "\n",
    "cur.execute(\"SELECT COUNT(user_id) from users;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_songplays(datapath: str) -> None:\n",
    "    from datetime import datetime\n",
    "    import re \n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    \n",
    "    def asdf(jsonfile):\n",
    "        for i in jsonfile:\n",
    "            if not i['userId']:\n",
    "                continue\n",
    "            if not i['level']:\n",
    "                continue\n",
    "            if not i['sessionId']:\n",
    "                continue\n",
    "            if not i['location']:\n",
    "                continue\n",
    "            if not i['userAgent']:\n",
    "                continue\n",
    "           \n",
    "            y = '|'.join(map(clean_csv_value, (\n",
    "                datetime.fromtimestamp(i['ts']/1000.0),\n",
    "                i['userId'],\n",
    "                i['level'],\n",
    "                r'\\N', #songid\n",
    "                r'\\N', #artistid\n",
    "                i['sessionId'], \n",
    "                i['location'],\n",
    "                i['userAgent'],\n",
    "                i['song'],\n",
    "                i['artist'],\n",
    "                i['length']\n",
    "            ))) + '\\n'\n",
    "            yield y\n",
    "   \n",
    "    x = StringIteratorIO(asdf(jsonfile))\n",
    "    cur.execute(\"\"\"DELETE FROM songplays;\"\"\")\n",
    "    cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_songplays;\n",
    "                SELECT * INTO tmp_songplays FROM songplays; \n",
    "                ALTER TABLE tmp_songplays \n",
    "                ADD COLUMN title VARCHAR, \n",
    "                ADD COLUMN artist VARCHAR, \n",
    "                ADD COLUMN duration DOUBLE PRECISION;\"\"\")\n",
    "    cur.copy_from(x, 'tmp_songplays', sep='|')\n",
    "    cur.execute(\"\"\"INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "                SELECT g.start_time, g.user_id, g.level, h.song_id, h.artist_id, g.session_id, g.location, g.user_agent FROM tmp_songplays g\n",
    "                LEFT JOIN (SELECT song_id, j.artist_id, k.name as artist, title, duration FROM songs j INNER JOIN artists k ON j.artist_id = k.artist_id) h \n",
    "                ON g.title = h.title AND g.duration = h.duration AND g.artist = h.artist\n",
    "                ON CONFLICT (start_time, user_id) DO NOTHING;\n",
    "                DROP TABLE tmp_songplays;\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Insight\n",
    "It took a while with A/B testing to figure this out. If I do a query for songid and artistid \n",
    "inside the StringIterator x it does not work as x will be used in `copy_from`. \n",
    "\n",
    "My solution: I do the filtering after `copy_from`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7752,)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = os.path.normcase(os.getcwd()) + '/data/log_data'\n",
    "process_songplays(datapath)\n",
    "\n",
    "cur.execute(\"SELECT COUNT(start_time) from songplays;\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Close Connection to Sparkify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Implement `etl.py`\n",
    "Use what you've completed in this notebook to implement `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "extra data after last expected column\nCONTEXT:  COPY tmp_songs, line 1: \"SOINLJW12A8C13314C|City Slickers|AR8IEZO1187B99055E|2008|149.86404\\nSOGDBUF12A8C140FAA|Intro|AR558FS...\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1755b5b8e962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1755b5b8e962>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormcase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/song_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mprocess_songs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1755b5b8e962>\u001b[0m in \u001b[0;36mprocess_songs\u001b[0;34m(cur, conn, datapath)\u001b[0m\n\u001b[1;32m    117\u001b[0m     cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_songs;\n\u001b[1;32m    118\u001b[0m                 SELECT * INTO tmp_songs FROM songs;\"\"\")\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tmp_songs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     cur.execute(\"\"\"INSERT INTO songs (song_id, title, artist_id, year, duration)\n\u001b[1;32m    121\u001b[0m                 \u001b[0mSELECT\u001b[0m \u001b[0msong_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martist_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mtmp_songs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: extra data after last expected column\nCONTEXT:  COPY tmp_songs, line 1: \"SOINLJW12A8C13314C|City Slickers|AR8IEZO1187B99055E|2008|149.86404\\nSOGDBUF12A8C140FAA|Intro|AR558FS...\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from typing import Iterator, Dict, Any, Optional\n",
    "from sql_queries import *\n",
    "\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=student password=student\")\n",
    "cur = conn.cursor()\n",
    "conn.set_session(autocommit=True)\n",
    "\n",
    "# get all files matching extension from directory\n",
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    # walk() generates the file names in a directory tree\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        # glob finds all the pathnames matching a specified pattern\n",
    "        # join combines the two path elements\n",
    "        files = glob.glob(os.path.join(root, '*.json'))\n",
    "        for f in files:\n",
    "            # add the absolute path to the list\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    return all_files\n",
    "\n",
    "\n",
    "def clean_csv_value(value: Optional[Any]) -> str:\n",
    "    r\"\"\"\n",
    "    Empty values are transformed to \\N. \n",
    "    The string \\N is the default string used by PostgreSQL to indicate \n",
    "    NULL in COPY (this can be changed using the NULL option).\n",
    "    \n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return r'\\N'\n",
    "    if value == 'NaN':\n",
    "        return r'\\N'        \n",
    "    \n",
    "    return str(value)\n",
    "\n",
    "\n",
    "class StringIteratorIO(io.TextIOBase):\n",
    "    \"\"\"\n",
    "    this class is copied from here: \n",
    "    https://hakibenita.com/fast-load-data-python-postgresql#copy-data-from-a-string-iterator\n",
    "    \n",
    "    thanks to the beer iterator the following class creates a file-like object that will act as a \n",
    "    buffer between the remote source and the COPY command. The buffer will consume \n",
    "    JSON via the iterator, clean and transform the data, and output clean CSV.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, iter: Iterator[str]):\n",
    "        self._iter = iter\n",
    "        self._buff = ''\n",
    "\n",
    "    def readable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _read1(self, n: Optional[int] = None) -> str:\n",
    "        while not self._buff:\n",
    "            try:\n",
    "                self._buff = next(self._iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        ret = self._buff[:n]\n",
    "        self._buff = self._buff[len(ret):]\n",
    "        return ret\n",
    "\n",
    "    def read(self, n: Optional[int] = None) -> str:\n",
    "        line = []\n",
    "        if n is None or n < 0:\n",
    "            while True:\n",
    "                m = self._read1()\n",
    "                if not m:\n",
    "                    break\n",
    "                line.append(m)\n",
    "        else:\n",
    "            while n > 0:\n",
    "                m = self._read1(n)\n",
    "                if not m:\n",
    "                    break\n",
    "                n -= len(m)\n",
    "                line.append(m)\n",
    "        return ''.join(line)\n",
    "\n",
    "\n",
    "\n",
    "def json_gen(file_list: list) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates a generator that reads a list of data paths and loads each json file as a \n",
    "    dictionary. If a json file has multiple dictionaries inside, it yields them separately.\n",
    "    \n",
    "    \"\"\"\n",
    "    import json\n",
    "    for file in file_list:    \n",
    "        with open(file) as json_file: \n",
    "            data = []\n",
    "            for line in json_file:\n",
    "                data = json.loads(line)\n",
    "                if not data:\n",
    "                    break\n",
    "                yield data\n",
    "            \n",
    "def process_songs(cur, conn, datapath: str) -> None:\n",
    "    file_list = get_files(datapath)\n",
    "    jsonfile = json_gen(file_list)\n",
    "    x = StringIteratorIO((\n",
    "        '|'.join(map(clean_csv_value, (\n",
    "            i['song_id'],\n",
    "            i['title'],\n",
    "            i['artist_id'],\n",
    "            i['year'],\n",
    "            i['duration']\n",
    "        ))) + r'\\n'\n",
    "        for i in jsonfile if i['song_id'] != ''\n",
    "    ))\n",
    "    cur.execute(\"\"\"DROP TABLE IF EXISTS tmp_songs;\n",
    "                SELECT * INTO tmp_songs FROM songs;\"\"\")\n",
    "    cur.copy_from(x, 'tmp_songs', sep='|')\n",
    "    cur.execute(\"\"\"INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
    "                SELECT song_id, title, artist_id, year, duration FROM tmp_songs\n",
    "                ON CONFLICT (song_id) DO NOTHING;\n",
    "                DROP TABLE tmp_songs;\"\"\")\n",
    "\n",
    "def main():\n",
    "    conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=student password=student\")\n",
    "    cur = conn.cursor()\n",
    "    conn.set_session(autocommit=True)\n",
    "    \n",
    "    filepath = os.path.normcase(os.getcwd()) + '/data/song_data'\n",
    "    process_songs(cur, conn, filepath)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
